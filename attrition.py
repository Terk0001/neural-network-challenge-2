# -*- coding: utf-8 -*-
"""attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VyDaKZvpL2ManmSpxRS05dSFHRZifqs6

## Part 1: Preprocessing
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras import layers

#  Import and read the attrition data
attrition_df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m19/lms/datasets/attrition.csv')
attrition_df.head()

# Determine the number of unique values in each column
attrition_df.nunique()

# Create y_df with the Attrition and Department columns
y_df = attrition_df[['Attrition', 'Department']]
y_df.head()

# Create a list of at least 10 column names to use as X data
column_names = ['Age', 'NumCompaniesWorked', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'YearsAtCompany']

# Create X_df using your selected columns
X_df = attrition_df[column_names]

# Show the data types for X_df
X_df.dtypes

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, random_state=42)

# Convert your X data to numeric data types however you see fit
# Add new code cells as necessary
print(X_train.dtypes)
print(X_test.dtypes)

# Create a StandardScaler
standard_scaler = StandardScaler()

# Fit the StandardScaler to the training data
X_scaler = standard_scaler.fit(X_train)

# Scale the training and testing data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

from sklearn.preprocessing import OneHotEncoder

# Create a OneHotEncoder for the Department column
d_encoder = OneHotEncoder()

# Fit the encoder to the training data
d_encoder.fit(y_train[['Department']])

# Create two new variables by applying the encoder
# to the training and testing data
y_d_train = d_encoder.transform(y_train[['Department']])
y_d_test = d_encoder.transform(y_test[['Department']])

y_d_train

# Create a OneHotEncoder for the Attrition column
ohe_attrition = OneHotEncoder()

# Fit the encoder to the training data
ohe_attrition.fit(y_train[['Attrition']])

# Create two new variables by applying the encoder
# to the training and testing data
y_train_attrition = ohe_attrition.transform(y_train[['Attrition']])
y_test_attrition = ohe_attrition.transform(y_test[['Attrition']])

y_train_attrition

"""## Part 2: Create, Compile, and Train the Model"""

# Find the number of columns in the X training data.
column_number = len(X_train.columns)

# Create the input layer
input_layer = layers.Input(shape=(column_number,), name = "input")

# Create at least two shared layers
shared_layer_1 = layers.Dense(32, activation = "relu", name = "shared_layer_1")(input_layer)
shared_layer_2 = layers.Dense(64, activation = "relu", name = "shared_layer_2")(shared_layer_1)

# Create a branch for Department
# with a hidden layer and an output layer

# Create the hidden layer
hidden_department_layer = layers.Dense(32, activation = "relu", name = "hidden_department")(shared_layer_2)

# Create the output layer
output_department_layer = layers.Dense(2, activation = "softmax", name = "output_department")(hidden_department_layer)

# Create a branch for Attrition
# with a hidden layer and an output layer

# Create the hidden layer
hidden_attrition_layer = layers.Dense(32, activation = "relu", name = "hidden_attrition")(shared_layer_2)

# Create the output layer
output_attrition_layer = layers.Dense(2, activation = "softmax", name = "output_attrition")(hidden_attrition_layer)

# Create the model
create_model = Model(inputs = input_layer, outputs = [output_department_layer, output_attrition_layer])

# Compile the model
create_model.compile(
    loss = {'output_department' : 'categorical_crossentropy',
            'output_attrition' : 'categorical_crossentropy'},
    optimizer = "adam",
    metrics = {'output_department' : 'accuracy',
               'output_attrition' : 'accuracy'}
)

# Summarize the model
create_model.summary()
# Summarize the model
create_model.summary()

# Train the model
fit_model = create_model.fit(
    X_train_scaled,
    {'output_department' : y_d_train, 'output_attrition' : y_train_attrition},
    epochs = 100
)

# Evaluate the model with the testing data
test_model = create_model.evaluate(X_test_scaled, {'output_department' : y_d_test, 'output_attrition' : y_test_attrition})

# Print the accuracy for both department and attrition
print(f"Department predictions accuracy: {test_model[3]}")
print(f"Attrition predictions accuracy: {test_model[4]}")

"""# Summary

In the provided space below, briefly answer the following questions.

1. Is accuracy the best metric to use on this data? Why or why not?

2. What activation functions did you choose for your output layers, and why?

3. Can you name a few ways that this model might be improved? Data can be optimized, different columns can be selected for train/test

YOUR ANSWERS HERE

1.
2.
3.
"""

